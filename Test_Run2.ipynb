{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# # Function to process tweet text\n",
    "# def process_tweet_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)  # Remove @ mentions\n",
    "#     text = text.strip(\" \")  # Remove whitespace resulting from above\n",
    "#     text = re.sub(r\" +\", \" \", text)  # Remove redundant spaces\n",
    "\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # Function to load tweets from dataset\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row[\"text\"]  # Assuming 'text' is the column name for tweet text\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "\n",
    "# def process_tweets_and_tokenize(tweets, max_sequence_length=None):\n",
    "#     # Process tweets\n",
    "#     processed_tweets = [process_tweet_text(tweet) for tweet in tweets]\n",
    "\n",
    "#     # Tokenize tweets\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_tweets)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_tweets)\n",
    "\n",
    "#     # Create vocabulary and reverse vocabulary\n",
    "#     word_index = tokenizer.word_index\n",
    "#     index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "#     # Compute vocabulary size\n",
    "#     vocab_size = len(word_index) + 1  # Add 1 for the reserved index 0\n",
    "\n",
    "#     # Pad sequences\n",
    "#     if max_sequence_length:\n",
    "#         padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "#     else:\n",
    "#         padded_sequences = pad_sequences(sequences, padding='post', truncating='post')\n",
    "\n",
    "#     return padded_sequences, word_index, index_word, vocab_size\n",
    "\n",
    "# # Example usage:\n",
    "# tweets = load_tweets_from_dataset(\"stockerbot-export.csv\")  # Replace with your dataset file\n",
    "# max_sequence_length = 200  # Choose your desired maximum sequence length\n",
    "# padded_sequences, word_index, index_word, vocab_size = process_tweets_and_tokenize(tweets, max_sequence_length)\n",
    "\n",
    "# # Generate input-output pairs\n",
    "# input_data = padded_sequences[:, :-1]  # Use all but the last token as input\n",
    "# output_data = padded_sequences[:, 1:]   # Shift by one to get the next token as output\n",
    "\n",
    "# # Reshape input data to include a batch dimension\n",
    "# input_data = np.expand_dims(input_data, axis=0)\n",
    "# output_data = np.expand_dims(output_data, axis=0)\n",
    "\n",
    "# print(\"Input shape:\", input_data.shape)\n",
    "# print(\"Output shape:\", output_data.shape)\n",
    "\n",
    "# # Reshape input data to include a batch dimension\n",
    "# train_input_data = np.expand_dims(input_data, axis=0)\n",
    "# val_input_data = np.expand_dims(val_input_data, axis=0)\n",
    "\n",
    "# # Reshape output data to include a batch dimension\n",
    "# train_output_data = np.expand_dims(output_data, axis=0)\n",
    "# val_output_data = np.expand_dims(val_output_data, axis=0)\n",
    "\n",
    "# # Split data into training and validation sets\n",
    "# train_input_data, val_input_data, train_output_data, val_output_data = train_test_split(\n",
    "#     input_data, output_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define model\n",
    "# with nengo.Network() as net:\n",
    "#     # Input layer\n",
    "#     inp = nengo.Node([0] * max_sequence_length)\n",
    "\n",
    "#     # Embedding layer\n",
    "#     embeddings = nengo_dl.Layer(tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "#                                                           output_dim=256))\n",
    "#     embedded = embeddings(inp)\n",
    "\n",
    "#     # Recurrent layer\n",
    "#     lstm = nengo_dl.Layer(tf.keras.layers.LSTM(units=12))\n",
    "#     rnn_output = lstm(embedded)\n",
    "\n",
    "#     # Output layer\n",
    "#     dense = nengo_dl.Layer(tf.keras.layers.Dense(units=vocab_size, activation='softmax'))\n",
    "#     out = dense(rnn_output)\n",
    "\n",
    "#     # Probe the output for training\n",
    "#     out_p = nengo.Probe(out)\n",
    "\n",
    "# # Convert the model to a NengoDL simulator\n",
    "# sim = nengo_dl.Simulator(net)\n",
    "\n",
    "# # Train the model\n",
    "# sim.compile(optimizer=tf.optimizers.Adam(), loss=tf.losses.CategoricalCrossentropy())\n",
    "# sim.fit(x=train_input_data, y=train_output_data, validation_data=(val_input_data, val_output_data), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEEMS TO RUN BUT NOT ENOUGH MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Function to process tweet text\n",
    "# def process_tweet_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)  # Remove @ mentions\n",
    "#     text = text.strip(\" \")  # Remove leading and trailing whitespace\n",
    "#     text = re.sub(r\" +\", \" \", text)  # Remove redundant spaces\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "# # Function to load tweets from dataset\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row[\"text\"]  # Assuming 'text' is the column name for tweet text\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "# def preprocess_tweets(processed_texts, max_sequence_length):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "#     sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding=\"post\")\n",
    "#     vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "#     return sequences_padded, vocab_size, tokenizer\n",
    "\n",
    "# # Load tweets from dataset\n",
    "# dataset_file = \"stockerbot-export.csv\"\n",
    "# tweets = load_tweets_from_dataset(dataset_file)\n",
    "\n",
    "# # Preprocess tweets with a maximum sequence length of 200 characters\n",
    "# X, vocab_size, tokenizer = preprocess_tweets(tweets, max_sequence_length=200)\n",
    "\n",
    "# # Split the data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert labels to categorical (one-hot encoding)\n",
    "# Y_train = to_categorical(X_train, num_classes=vocab_size)\n",
    "# Y_test = to_categorical(X_test, num_classes=vocab_size)\n",
    "\n",
    "# # Define Model\n",
    "# with nengo.Network(seed=0) as net:\n",
    "#     neuron_type = nengo.LIF(amplitude=0.01)  # Define the neuron type\n",
    "\n",
    "#     # Define input node\n",
    "#     # Ensure the input node is correctly defined (flatten if necessary)\n",
    "#     input_node = nengo.Node(output=np.zeros(X_train.shape[1]))\n",
    "\n",
    "#     # Embedding layer\n",
    "#     # Apply neuron type after embedding layer if desired\n",
    "#     emb = nengo_dl.Layer(tf.keras.layers.Embedding(vocab_size, 64))(input_node)\n",
    "#     emb = nengo_dl.Layer(neuron_type)(emb)\n",
    "\n",
    "#     # LSTM layer\n",
    "#     # LSTM processing with neuron dynamics applied afterwards\n",
    "#     lstm = nengo_dl.Layer(tf.keras.layers.LSTM(64))(emb)\n",
    "#     lstm = nengo_dl.Layer(neuron_type)(lstm)\n",
    "\n",
    "#     # Output layer\n",
    "#     # It is common to not apply spiking neurons right before the output layer in classification tasks\n",
    "#     out = nengo_dl.Layer(tf.keras.layers.Dense(vocab_size, activation='softmax'))(lstm)\n",
    "\n",
    "#     # Probes\n",
    "#     output_p = nengo.Probe(out, label=\"out_probe\")\n",
    "\n",
    "# # Simulator\n",
    "# minibatch_size = 256\n",
    "# sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)\n",
    "\n",
    "# sim.compile(\n",
    "#     optimizer=tf.optimizers.Adam(),\n",
    "#     loss=tf.losses.CategoricalCrossentropy(),\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# # Fit model\n",
    "# sim.fit(X_train, Y_train, epochs=5, validation_data=(X_test, Y_test))\n",
    "\n",
    "# # Function to generate text\n",
    "# def generate_text(seed_text, num_words, model, tokenizer, max_sequence_length):\n",
    "#     for _ in range(num_words):\n",
    "#         token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "#         token_list = pad_sequences([token_list], maxlen=max_sequence_length, padding='post')\n",
    "#         predicted_probs = model.predict(token_list, verbose=0)\n",
    "#         predicted_index = np.argmax(predicted_probs, axis=-1)\n",
    "#         predicted_word = tokenizer.index_word[predicted_index[0]]\n",
    "#         seed_text += \" \" + predicted_word\n",
    "#     return seed_text\n",
    "\n",
    "# # Generate text\n",
    "# seed_text = \"Deep learning models\"\n",
    "# generated_text = generate_text(seed_text, 10, sim, tokenizer, 200)\n",
    "# print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
