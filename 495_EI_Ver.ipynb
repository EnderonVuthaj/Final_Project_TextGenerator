{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to process tweet text\n",
    "# def process_tweet_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)  # Remove @ mentions\n",
    "#     text = text.strip(\" \")  # Remove whitespace resulting from above\n",
    "#     text = re.sub(r\" +\", \" \", text)  # Remove redundant spaces\n",
    "\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # Function to load tweets from dataset\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row[\"text\"]  # Assuming 'text' is the column name for tweet text\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "\n",
    "# def preprocess_tweets(processed_texts, max_sequence_length):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "\n",
    "#     # Pad sequences to the specified maximum length\n",
    "#     sequences_padded = pad_sequences(\n",
    "#         sequences, maxlen=max_sequence_length, padding=\"pre\"\n",
    "#     )\n",
    "\n",
    "#     vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "\n",
    "#     return sequences_padded, vocab_size\n",
    "\n",
    "# # Load tweets from dataset\n",
    "# dataset_file = \"stockerbot-export.csv\"\n",
    "# tweets = load_tweets_from_dataset(dataset_file)\n",
    "\n",
    "# # Preprocess tweets with a maximum sequence length of 200 characters\n",
    "# X, vocab_size = preprocess_tweets(tweets, max_sequence_length=200)\n",
    "\n",
    "# # Split the data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Reshape input data to include batch dimension and set number of steps to 1\n",
    "# X_train_reshaped = X_train[:, np.newaxis, :]\n",
    "\n",
    "# # Repeat the test data along the time axis\n",
    "# X_test_reshaped = np.tile(X_test[:, np.newaxis, :], (1, 40, 1))\n",
    "\n",
    "# print(\"Shapes after preprocessing:\")\n",
    "# print(\"X shape:\", X.shape)\n",
    "# print(\"X_train_reshaped shape:\", X_train_reshaped.shape)\n",
    "# print(\"X_test_reshaped shape:\", X_test_reshaped.shape)\n",
    "# print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the NengoDL model\n",
    "# with nengo.Network() as net:\n",
    "\n",
    "#     neuron_type = nengo.LIF(amplitude=0.01)\n",
    "\n",
    "#     # Define input node\n",
    "#     input_node = nengo.Node(np.zeros(X_train_reshaped.shape[2]))\n",
    "\n",
    "#     print(\"input node\", input_node)\n",
    "\n",
    "#     # Embedding layer\n",
    "#     a = nengo_dl.Layer(\n",
    "#         tf.keras.layers.Embedding(vocab_size, 256)\n",
    "#     )(input_node) #X_train_reshaped.shape[2], 64\n",
    "#     a = nengo_dl.Layer(neuron_type)(a)\n",
    "\n",
    "#     print(\"a node\", a)\n",
    "\n",
    "#     # LSTM layer\n",
    "#     b = nengo_dl.Layer(tf.keras.layers.LSTM(256))(a)\n",
    "#     b = nengo_dl.Layer(neuron_type)(b)\n",
    "\n",
    "#     print(\"b node\", b)\n",
    "\n",
    "#     # Dropout layer\n",
    "#     # c = nengo_dl.Layer(tf.keras.layers.Dropout(0.2))(b)  # Adding dropout\n",
    "#     # c = nengo_dl.Layer(neuron_type)(c)\n",
    "\n",
    "#     # print(\"c node\", c)\n",
    "\n",
    "#     # Final dense layer\n",
    "#     out = nengo_dl.Layer(tf.keras.layers.Dense(vocab_size, activation='softmax'))(b)\n",
    "\n",
    "#     # Probe the output node\n",
    "#     output_p = nengo.Probe(out)\n",
    "\n",
    "# print(\"\\nArchitecture:\")\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch_size = 256\n",
    "# sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim.compile(\n",
    "#             optimizer=tf.optimizers.Adam(),\n",
    "#             loss=tf.losses.CategoricalCrossentropy(),\n",
    "#             metrics=[\"accuracy\"],\n",
    "#         )\n",
    "\n",
    "# sim.fit(X_train_reshaped, {out: X_train_reshaped}, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATGPT ---- vector error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Process tweet text function\n",
    "# def process_tweet_text(text):\n",
    "#     # Regular expressions for cleaning text\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)\n",
    "#     text = text.strip()\n",
    "#     text = re.sub(r\" +\", \" \", text)\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "# # Load and preprocess tweets\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row['text']\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "# # Tokenize and pad sequences\n",
    "# def preprocess_tweets(processed_texts, max_sequence_length):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "#     sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
    "#     vocab_size = len(tokenizer.word_index) + 1\n",
    "#     return sequences_padded, vocab_size, tokenizer\n",
    "\n",
    "# # Load tweets and preprocess\n",
    "# dataset_file = \"stockerbot-export.csv\"\n",
    "# tweets = load_tweets_from_dataset(dataset_file)\n",
    "# X, vocab_size, tokenizer = preprocess_tweets(tweets, max_sequence_length=200)\n",
    "# X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the NengoDL model\n",
    "# with nengo.Network() as net:\n",
    "#     nengo_dl.configure_settings(trainable=True)\n",
    "\n",
    "#     # Input node shaped to match input data\n",
    "#     input_node = nengo.Node(np.zeros((X_train.shape[1], 1)))\n",
    "\n",
    "#     # Define layers with explicit shapes\n",
    "#     emb = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128),\n",
    "#         shape_in=(1,),  # Single integer per timestep\n",
    "#         shape_out=(128,)\n",
    "#     )\n",
    "#     nengo.Connection(input_node, emb, synapse=None)\n",
    "\n",
    "#     lstm1 = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "#         shape_in=(128,),\n",
    "#         shape_out=(64,)\n",
    "#     )\n",
    "#     nengo.Connection(emb, lstm1, synapse=None)\n",
    "\n",
    "#     lstm2 = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "#         shape_in=(64,),\n",
    "#         shape_out=(64,)\n",
    "#     )\n",
    "#     nengo.Connection(lstm1, lstm2, synapse=None)\n",
    "\n",
    "#     lstm3 = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.LSTM(64),\n",
    "#         shape_in=(64,),\n",
    "#         shape_out=(64,)\n",
    "#     )\n",
    "#     nengo.Connection(lstm2, lstm3, synapse=None)\n",
    "\n",
    "#     dense = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         shape_in=(64,),\n",
    "#         shape_out=(128,)\n",
    "#     )\n",
    "#     nengo.Connection(lstm3, dense, synapse=None)\n",
    "\n",
    "#     output = nengo_dl.TensorNode(\n",
    "#         tf.keras.layers.Dense(vocab_size, activation='softmax'),\n",
    "#         shape_in=(128,),\n",
    "#         shape_out=(vocab_size,)\n",
    "#     )\n",
    "#     nengo.Connection(dense, output, synapse=None)\n",
    "\n",
    "#     output_probe = nengo.Probe(output)\n",
    "\n",
    "# # Simulation and training\n",
    "# with nengo_dl.Simulator(net, minibatch_size=128) as sim:\n",
    "#     sim.compile(optimizer=tf.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "#     sim.fit({input_node: X_train}, {output_probe: X_train}, epochs=10)\n",
    "\n",
    "# # Function to generate text\n",
    "# def generate_text(sim, seed_text, num_words, tokenizer):\n",
    "#     for _ in range(num_words):\n",
    "#         encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "#         padded_sequence = pad_sequences([encoded], maxlen=200, truncating='pre')\n",
    "#         prediction = sim.predict_on_batch({input_node: padded_sequence})\n",
    "#         predicted_index = np.argmax(prediction[output_probe][0], axis=-1)\n",
    "#         output_word = tokenizer.index_word[predicted_index]\n",
    "#         seed_text += \" \" + output_word\n",
    "#     return seed_text\n",
    "\n",
    "# # Generate text using the trained model\n",
    "# seed_text = \"initial words of your tweet\"\n",
    "# generated_text = generate_text(sim, seed_text, 50, tokenizer)\n",
    "# print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
