{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after preprocessing:\n",
      "X shape: (28276, 200)\n",
      "X_train_reshaped shape: (22620, 1, 200)\n",
      "X_test_reshaped shape: (5656, 40, 200)\n",
      "Vocabulary size: 23517\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Function to process tweet text\n",
    "# def process_tweet_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)  # Remove @ mentions\n",
    "#     text = text.strip(\" \")  # Remove whitespace resulting from above\n",
    "#     text = re.sub(r\" +\", \" \", text)  # Remove redundant spaces\n",
    "\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # Function to load tweets from dataset\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row[\"text\"]  # Assuming 'text' is the column name for tweet text\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "\n",
    "# def preprocess_tweets(processed_texts, max_sequence_length):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "\n",
    "#     # Pad sequences to the specified maximum length\n",
    "#     sequences_padded = pad_sequences(\n",
    "#         sequences, maxlen=max_sequence_length, padding=\"pre\"\n",
    "#     )\n",
    "\n",
    "#     vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "\n",
    "#     return sequences_padded, vocab_size\n",
    "\n",
    "# # Load tweets from dataset\n",
    "# dataset_file = \"stockerbot-export.csv\"\n",
    "# tweets = load_tweets_from_dataset(dataset_file)\n",
    "\n",
    "# # Preprocess tweets with a maximum sequence length of 200 characters\n",
    "# X, vocab_size = preprocess_tweets(tweets, max_sequence_length=200)\n",
    "\n",
    "# # Split the data into training and testing sets (80% training, 20% testing)\n",
    "# X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Reshape input data to include batch dimension and set number of steps to 1\n",
    "# X_train_reshaped = X_train[:, np.newaxis, :]\n",
    "\n",
    "# # Repeat the test data along the time axis\n",
    "# X_test_reshaped = np.tile(X_test[:, np.newaxis, :], (1, 40, 1))\n",
    "\n",
    "# print(\"Shapes after preprocessing:\")\n",
    "# print(\"X shape:\", X.shape)\n",
    "# print(\"X_train_reshaped shape:\", X_train_reshaped.shape)\n",
    "# print(\"X_test_reshaped shape:\", X_test_reshaped.shape)\n",
    "# print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nengo.Network(seed=0) as net:\n",
    "\n",
    "#     # NengoDL default parameters for the neurons that will make\n",
    "#     # the training progress more smoothly\n",
    "#     net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
    "#     net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "#     net.config[nengo.Connection].synapse = None\n",
    "#     neuron_type = nengo.LIF(amplitude=0.01)\n",
    "#     nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "#     # This is the input node that will be used to rate encode in input images (i.e. our Flatten layer from ICE1)\n",
    "#     inp = nengo.Node(np.zeros(X_train_reshaped.shape[2]))\n",
    "\n",
    "#     # First dense layer\n",
    "#     a = nengo_dl.Layer(\n",
    "#         tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "#     )(inp)\n",
    "#     a = nengo_dl.Layer(neuron_type)(a)\n",
    "\n",
    "#     # Dropout layer\n",
    "#     b = nengo_dl.Layer(tf.keras.layers.Dropout(0.2))(a)\n",
    "#     b = nengo_dl.Layer(neuron_type)(b)\n",
    "\n",
    "#     # Final dense layer\n",
    "#     out = nengo_dl.Layer(tf.keras.layers.Dense(vocab_size))(b)\n",
    "\n",
    "#     # Here we create two different output probes, one with a filter\n",
    "#     # (for when we're simulating the network over time and\n",
    "#     # accumulating spikes), and one without (for when we're\n",
    "#     # training the network using a rate-based approximation)\n",
    "#     out_p = nengo.Probe(out, label=\"out_p\")\n",
    "#     out_p_filt = nengo.Probe(out, synapse=0.1, label=\"out_p_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                     Building network (0%)                    | ETA:  --:--:--\n",
      "|#####################Building network (85%)###########          | ETA: 0:00:00\n",
      "|#####################Building network (85%)###########          | ETA: 0:00:00\n",
      "Build finished in 0:00:00\n",
      "|#                         Optimizing graph                           | 0:00:00\n",
      "|#             Optimizing graph: operator simplificaton               | 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:456: UserWarning: No GPU support detected. See https://www.nengo.ai/nengo-dl/installation.html#installing-tensorflow for instructions on setting up TensorFlow with GPU support.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing graph: operator simplificaton finished in 0:00:00\n",
      "|#                Optimizing graph: merging operators                 | 0:00:00\n",
      "Optimizing graph: merging operators finished in 0:00:00\n",
      "|#                Optimizing graph: ordering signals                  | 0:00:00\n",
      "Optimizing graph: ordering signals finished in 0:00:00\n",
      "|#                Optimizing graph: creating signals                  | 0:00:00\n",
      "Optimizing graph: creating signals finished in 0:00:00\n",
      "Optimization finished in 0:00:00\n",
      "|#                        Constructing graph                          | 0:00:00\n",
      "|#                        Constructing graph                          | 0:00:00\n",
      "|  #                      Constructing graph                          | 0:00:00\n",
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n",
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|########      Constructing graph: build stage (13%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (53%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (86%)####         | ETA: 0:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "|     #                   Constructing graph                          | 0:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n",
      "Construction finished in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# minibatch_size = 256\n",
    "# sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:1736: UserWarning: Number of elements (1) in ['SparseCategoricalAccuracy'] does not match number of Probes (2); consider using an explicit input dictionary in this case, so that the assignment of data to objects is unambiguous.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Custom loss function for testing\n",
    "# def classification_accuracy(y_true, y_pred):\n",
    "#     return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])\n",
    "\n",
    "\n",
    "# # Compile for training\n",
    "# sim.compile(\n",
    "#     optimizer=tf.optimizers.Adam(),\n",
    "#     loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "#     metrics=[tf.metrics.SparseCategoricalAccuracy()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:1892: UserWarning: Number of elements in input data (22620) is not evenly divisible by Simulator.minibatch_size (256); input data will be truncated.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|############  Constructing graph: build stage (20%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (53%)             | ETA: 0:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 962, in sparse_categorical_matches  **\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[2], expected a dimension of 1, got 200 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_2)' with input shapes: [256,1,200].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mout_p\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo\\utils\\magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:63\u001b[0m, in \u001b[0;36mrequire_open\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SimulatorClosed(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after simulator is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:858\u001b[0m, in \u001b[0;36mSimulator.fit\u001b[1;34m(self, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (x_val, y_val, validation_data[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m--> 858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_keras(\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, n_steps\u001b[38;5;241m=\u001b[39mn_steps, stateful\u001b[38;5;241m=\u001b[39mstateful, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    860\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo\\utils\\magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:46\u001b[0m, in \u001b[0;36mwith_self\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(instance\u001b[38;5;241m.\u001b[39mtensor_graph\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m---> 46\u001b[0m         output \u001b[38;5;241m=\u001b[39m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mset_floatx(keras_dtype)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:1022\u001b[0m, in \u001b[0;36mSimulator._call_keras\u001b[1;34m(self, func_type, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     func_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1022\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model, func_type)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_args)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# update n_steps/time\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stateful:\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\C25END~1.XK\\AppData\\Local\\Temp\\__autograph_generated_fileq3xgoknc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 143, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 700, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 962, in sparse_categorical_matches  **\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[2], expected a dimension of 1, got 200 for '{{node Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_2)' with input shapes: [256,1,200].\n"
     ]
    }
   ],
   "source": [
    "# sim.fit(X_train_reshaped, {out_p: X_train_reshaped}, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|           Constructing graph: pre-build stage (0%)           | ETA:  --:--:--\n",
      "Constructing graph: pre-build stage finished in 0:00:00\n",
      "|             Constructing graph: build stage (0%)             | ETA:  --:--:--\n",
      "|############  Constructing graph: build stage (20%)             | ETA: 0:00:00\n",
      "|##############Constructing graph: build stage (86%)####         | ETA: 0:00:00\n",
      "|############Constructing graph: build stage (100%)############| ETA:  00:00:00\n",
      "Constructing graph: build stage finished in 0:00:00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Temp\\ipykernel_8616\\4287898799.py\", line 3, in classification_accuracy  *\n        return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 200 for '{{node classification_accuracy/Squeeze}} = Squeeze[T=DT_INT32, squeeze_dims=[-1]](classification_accuracy/strided_slice)' with input shapes: [256,200].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sim\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m{out_p_filt: classification_accuracy})\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy after training:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 4\u001b[0m     \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test_reshaped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mout_p_filt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_reshaped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# sim.evaluate(test_images, {???: test_labels}, verbose=0)[\"loss\"],\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo\\utils\\magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:63\u001b[0m, in \u001b[0;36mrequire_open\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SimulatorClosed(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after simulator is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m     )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:895\u001b[0m, in \u001b[0;36mSimulator.evaluate\u001b[1;34m(self, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;129m@require_open\u001b[39m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;129m@fill_docs\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstateful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stateful\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m    Compute the loss and metric values for the network.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m        ``outputs[\"probe_name_loss\"]`` or ``outputs[\"probe_name_metric_name\"]``.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_keras(\n\u001b[0;32m    896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, n_steps\u001b[38;5;241m=\u001b[39mn_steps, stateful\u001b[38;5;241m=\u001b[39mstateful, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    897\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo\\utils\\magic.py:179\u001b[0m, in \u001b[0;36mBoundFunctionWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(wrapped, instance, args, kwargs)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__wrapped__, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:46\u001b[0m, in \u001b[0;36mwith_self\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(instance\u001b[38;5;241m.\u001b[39mtensor_graph\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m---> 46\u001b[0m         output \u001b[38;5;241m=\u001b[39m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mset_floatx(keras_dtype)\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nengo_dl\\simulator.py:1022\u001b[0m, in \u001b[0;36mSimulator._call_keras\u001b[1;34m(self, func_type, x, y, n_steps, stateful, **kwargs)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     func_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1022\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeras_model, func_type)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_args)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# update n_steps/time\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stateful:\n",
      "File \u001b[1;32mc:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\C25END~1.XK\\AppData\\Local\\Temp\\__autograph_generated_filemyhzc139.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\C25END~1.XK\\AppData\\Local\\Temp\\__autograph_generated_filerd3v51iq.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__classification_accuracy\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39msparse_categorical_accuracy, (ag__\u001b[38;5;241m.\u001b[39mld(y_true)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(y_pred)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Temp\\ipykernel_8616\\4287898799.py\", line 3, in classification_accuracy  *\n        return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\metrics\\metrics.py\", line 3669, in sparse_categorical_accuracy\n        matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n    File \"c:\\Users\\C25Enderon.Vuthaj.XK\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 962, in sparse_categorical_matches\n        y_true = tf.squeeze(y_true, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 200 for '{{node classification_accuracy/Squeeze}} = Squeeze[T=DT_INT32, squeeze_dims=[-1]](classification_accuracy/strided_slice)' with input shapes: [256,200].\n"
     ]
    }
   ],
   "source": [
    "# sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "# print(\n",
    "#     \"Accuracy after training:\",\n",
    "#     sim.evaluate(\n",
    "#         X_test_reshaped[0 : 256 * 3], {out_p_filt: X_test_reshaped[0 : 256 * 3]}, verbose=0\n",
    "#     )[\"loss\"],\n",
    "#     # sim.evaluate(test_images, {???: test_labels}, verbose=0)[\"loss\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import re\n",
    "# import nengo\n",
    "# import nengo_dl\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Function to process tweet text\n",
    "# def process_tweet_text(text):\n",
    "#     text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "#     text = re.sub(r\"@[a-zA-Z0-9_]+\", \"\", text)  # Remove @ mentions\n",
    "#     text = text.strip(\" \")  # Remove whitespace resulting from above\n",
    "#     text = re.sub(r\" +\", \" \", text)  # Remove redundant spaces\n",
    "\n",
    "#     text = re.sub(r\"&lt;\", \"<\", text)\n",
    "#     text = re.sub(r\"&gt;\", \">\", text)\n",
    "#     text = re.sub(r\"&amp;\", \"&\", text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # Function to load tweets from dataset\n",
    "# def load_tweets_from_dataset(dataset_file):\n",
    "#     tweets = []\n",
    "#     with open(dataset_file, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             text = row[\"text\"]  # Assuming 'text' is the column name for tweet text\n",
    "#             processed_text = process_tweet_text(text)\n",
    "#             tweets.append(processed_text)\n",
    "#     return tweets\n",
    "\n",
    "\n",
    "# # Function to preprocess tweets\n",
    "# def preprocess_tweets(processed_texts):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(processed_texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "\n",
    "#     max_sequence_length = max([len(seq) for seq in sequences])\n",
    "#     sequences_padded = pad_sequences(\n",
    "#         sequences, maxlen=max_sequence_length, padding=\"pre\"\n",
    "#     )\n",
    "\n",
    "#     vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "\n",
    "#     return sequences_padded, vocab_size\n",
    "\n",
    "\n",
    "# # # Load tweets from dataset\n",
    "# # dataset_file = \"stockerbot-export.csv\"\n",
    "# # tweets = load_tweets_from_dataset(dataset_file)\n",
    "\n",
    "# # Preprocess tweets\n",
    "# X, vocab_size = preprocess_tweets(tweets)\n",
    "\n",
    "# # Define the NengoDL model\n",
    "# with nengo.Network() as net:\n",
    "#     # Define input node\n",
    "#     input_node = nengo.Node([0] * X.shape[1])  # Adjusted to match the input shape\n",
    "\n",
    "#     # Embedding layer\n",
    "#     a = nengo_dl.Layer(\n",
    "#         tf.keras.layers.Embedding(vocab_size, 256, input_length=X.shape[1])\n",
    "#     )(input_node)\n",
    "\n",
    "#     # LSTM layer\n",
    "#     b = nengo_dl.Layer(tf.keras.layers.LSTM(256))(a)\n",
    "\n",
    "#     # Dropout layer\n",
    "#     b = nengo_dl.Layer(tf.keras.layers.Dropout(0.5))(b)  # Adding dropout\n",
    "\n",
    "#     # Final dense layer\n",
    "#     out = nengo_dl.Layer(tf.keras.layers.Dense(vocab_size, activation='softmax'))(b)\n",
    "\n",
    "#     # Probe the output node\n",
    "#     output_p = nengo.Probe(out)\n",
    "\n",
    "# # Print architecture\n",
    "# print(\"\\nArchitecture:\")\n",
    "# print(net)\n",
    "\n",
    "# # Compile the model\n",
    "# try:\n",
    "#     with nengo_dl.Simulator(net) as sim:\n",
    "#         sim.compile(\n",
    "#             optimizer=tf.optimizers.Adam(),\n",
    "#             loss=tf.losses.CategoricalCrossentropy(),\n",
    "#             metrics=[\"accuracy\"],\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         sim.fit(\n",
    "#             {input_node: X},\n",
    "#             {output_p: np.zeros((X.shape[0], 1, vocab_size))},  # Dummy targets for unsupervised learning\n",
    "#             epochs=10,\n",
    "#             batch_size=64,\n",
    "#         )\n",
    "\n",
    "#         # Check training progress\n",
    "#         print(\"\\nTraining completed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError during model training: {e}\")\n",
    "\n",
    "\n",
    "# # def generate_tweet(model, tokenizer, seed_text, max_length=50):\n",
    "# #     for _ in range(max_length):\n",
    "# #         token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "# #         token_list = pad_sequences([token_list], maxlen=max_length-1, padding='pre')\n",
    "# #         predicted = model.predict(token_list, verbose=0)\n",
    "# #         next_token = np.argmax(predicted)\n",
    "# #         next_word = tokenizer.index_word[next_token]\n",
    "# #         seed_text += \" \" + next_word\n",
    "# #         if next_word == 'endseq':\n",
    "# #             break\n",
    "# #     return seed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
